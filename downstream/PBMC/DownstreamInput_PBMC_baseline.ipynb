{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 01:40:51.655640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 01:40:56.115095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import torch\n",
    "import torch_cluster\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from sc_model import *\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f'data_su2020/'\n",
    "target_dir = f'{base_path}/VAE/'\n",
    "data_dir = f'{base_path}/data/'\n",
    "\n",
    "# dir_path=\"covid\"\n",
    "# base_path = f\"../ProtoCell4P-main/data/{dir_path}/\"\n",
    "# target_dir = f'{base_path}/VAE/'\n",
    "# dir create\n",
    "# os.makedirs(target_dir, exist_ok=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset_and_preprocessors(base_path, exp, device):\n",
    "    train_dataset = torch.load(f\"{base_path}/train_dataset_exp{exp}_HVG_count_noflt_only2.pt\", map_location= device)\n",
    "    val_dataset = torch.load(f\"{base_path}/val_dataset_exp{exp}_HVG_count_noflt_only2.pt\", map_location= device)\n",
    "    test_dataset = torch.load(f\"{base_path}/test_dataset_exp{exp}_HVG_count_noflt_only2.pt\",map_location = device)\n",
    "    \n",
    "    with open(f\"{base_path}/label_encoder_exp{exp}_HVG_count_noflt_only2.pkl\", 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    with open(f\"{base_path}/scaler_exp{exp}_HVG_count_noflt_only2.pkl\", 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, label_encoder, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device: cuda:4\n"
     ]
    }
   ],
   "source": [
    "device_num = 4\n",
    "device = torch.device(f'cuda:{device_num}' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"INFO: Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceDataset2(Dataset):\n",
    "    '''\n",
    "    인스턴스 단위로 데이터를 반환하는 Dataset 클래스.\n",
    "    MilDataset과 유사하지만, 각 인스턴스에 대한 데이터와 레이블을 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        data (Tensor): 특성 데이터\n",
    "        ids (Tensor): 각 인스턴스에 대응하는 백의 ID\n",
    "        labels (Tensor): 각 백에 대한 레이블\n",
    "        instance_labels (Tensor): 각 인스턴스에 대한 레이블\n",
    "        normalize (bool): 데이터 정규화 여부\n",
    "    '''\n",
    "    def __init__(self, data, ids, labels, instance_labels, bag_labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        self.mil_ids = ids.clone()\n",
    "        self.instance_labels = instance_labels\n",
    "        self.bag_labels = bag_labels\n",
    "        if (len(self.mil_ids.shape) == 1):\n",
    "            self.mil_ids.resize_(1, len(self.mil_ids))\n",
    "        self.bags = torch.unique(self.mil_ids[0])\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "    def __getitem__(self, index):\n",
    "        # 각 인스턴스에 대한 데이터와 레이블을 반환\n",
    "        data = self.data[index]\n",
    "        bag_id = self.ids[index] \n",
    "        \n",
    "        instance_label = self.instance_labels[index]\n",
    "        bag_label = self.bag_labels[index]\n",
    "        return data, bag_id, instance_label, bag_label\n",
    "    \n",
    "\n",
    "def update_instance_labels_with_bag_labels(instance_dataset):\n",
    "    \"\"\"\n",
    "    Updates the instance labels in the InstanceDataset with the corresponding bag labels.\n",
    "    \n",
    "    Args:\n",
    "    instance_dataset (InstanceDataset): The dataset whose instance labels are to be updated.\n",
    "    \n",
    "    Note: This function modifies the instance_dataset in-place.\n",
    "    \"\"\"\n",
    "    combined_labels = torch.empty(len(instance_dataset), dtype=torch.long, device=device)\n",
    "    for i in range(len(instance_dataset)):\n",
    "        _, bag_id, instance_label = instance_dataset[i]\n",
    "        bag_index = (instance_dataset.bags == bag_id).nonzero(as_tuple=True)[0][0]\n",
    "\n",
    "        bag_label = instance_dataset.labels[bag_index]\n",
    "\n",
    "        combined_label = instance_label * 0 + bag_label\n",
    "        combined_labels[i] = combined_label \n",
    "        \n",
    "    return InstanceDataset2(instance_dataset.data, instance_dataset.ids, instance_dataset.labels, instance_dataset.instance_labels, combined_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, L, D, K):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, H):\n",
    "        A = self.attention(H)  # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        # A = F.softmax(A, dim=1)  # softmax over N\n",
    "        return A\n",
    "class GatedAttentionModule(nn.Module):\n",
    "    def __init__(self, L, D, K):\n",
    "        super(GatedAttentionModule, self).__init__()\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_weights = nn.Linear(self.D, self.K)\n",
    "\n",
    "    def forward(self, H):\n",
    "        A_V = self.attention_V(H)  # NxD\n",
    "        A_U = self.attention_U(H)  # NxD\n",
    "        A = self.attention_weights(A_V * A_U)  # element wise multiplication # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        # A = F.softmax(A, dim=1)  # softmax over N\n",
    "        return A\n",
    "\n",
    "\n",
    "class TeacherBranch(nn.Module):\n",
    "  def __init__(self, input_dims, latent_dims, attention_module, \n",
    "               num_classes=2, \n",
    "               activation_function=nn.Sigmoid, \n",
    "               dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.input_dims = input_dims\n",
    "    self.L = latent_dims\n",
    "    self.K = 1\n",
    "    self.D = latent_dims\n",
    "    self.attention_module = attention_module\n",
    "    self.num_classes = num_classes\n",
    "    \n",
    "    self.bagNN = nn.Sequential(\n",
    "        nn.Linear(self.input_dims, self.L),\n",
    "        \n",
    "        activation_function(),\n",
    "        nn.Linear(self.L, self.L),\n",
    "        activation_function(),\n",
    "        # mode 1\n",
    "        # nn.Linear(self.L, self.L//4),\n",
    "        # activation_function(),\n",
    "        \n",
    "        # nn.Linear(self.L//4, self.L//4),\n",
    "        # activation_function(),\n",
    "        \n",
    "        # nn.Linear(self.L//4, self.num_classes ),\n",
    "        \n",
    "        # mode 2\n",
    "        nn.Linear(self.L, self.num_classes ),\n",
    "    )\n",
    "    self.initialize_weights()\n",
    "      \n",
    "  def forward(self, input, replaceAS=None):  \n",
    "    if replaceAS is not None:\n",
    "      attention_weights = F.softmax(replaceAS,dim=1)\n",
    "    else:\n",
    "      attention_weights = self.attention_module(input)\n",
    "      attention_weights = F.softmax(attention_weights,dim=1)\n",
    "    \n",
    "    aggregated_instance = torch.mm(attention_weights, input)\n",
    "    output = aggregated_instance.squeeze()\n",
    "    output = self.bagNN(output)\n",
    "    return output\n",
    "  \n",
    "  def initialize_weights(self):\n",
    "      for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "\n",
    "class StudentBranch(nn.Module):\n",
    "  def __init__(self, input_dims, latent_dims, \n",
    "               num_classes=2, \n",
    "               activation_function=nn.ReLU):\n",
    "    super().__init__()\n",
    "    self.input_dims = input_dims\n",
    "    self.L = latent_dims\n",
    "    self.K = 1\n",
    "    self.D = latent_dims\n",
    "    self.num_classes = num_classes \n",
    "    \n",
    "    self.instanceNN = nn.Sequential(\n",
    "        nn.Linear(self.input_dims, self.L),\n",
    "        activation_function(),\n",
    "        # mode 1\n",
    "        # nn.Linear(self.L, self.L//4),\n",
    "        # activation_function(),\n",
    "        \n",
    "        # nn.Linear(self.L//4, self.L//4),\n",
    "        # activation_function(),\n",
    "        \n",
    "        # nn.Linear(self.L//4, self.num_classes ),\n",
    "        # mode 2\n",
    "        nn.Linear(self.L, self.L),\n",
    "        activation_function(),\n",
    "        nn.Linear(self.L, self.num_classes )\n",
    "      )\n",
    "    self.initialize_weights()\n",
    "  def forward(self, input):  \n",
    "    NN_out = input\n",
    "    output = self.instanceNN(NN_out)\n",
    "    \n",
    "    return output #, norm_attention_score\n",
    "  \n",
    "  def initialize_weights(self):\n",
    "      for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "\n",
    "class EncoderBranch(nn.Module):\n",
    "  def __init__(self, proto_vae, output_dims, activation_function = nn.Tanh):\n",
    "    super().__init__()\n",
    "    self.proto_vae = proto_vae\n",
    "    self.activation_function = activation_function\n",
    "    self.output_dims = output_dims\n",
    "    self.encoder_layer = nn.Sequential(\n",
    "      nn.Linear(self.proto_vae.latent_dim, self.output_dims),\n",
    "      activation_function(),\n",
    "      nn.Linear(self.output_dims, self.output_dims),\n",
    "      activation_function(),\n",
    "      nn.Linear(self.output_dims, self.output_dims)\n",
    "    )\n",
    "    self.initialize_weights()\n",
    "  def forward(self, input):\n",
    "    with torch.no_grad():\n",
    "      vae_latent = self.proto_vae.features(input)\n",
    "      mu = vae_latent[:,:self.proto_vae.latent_dim]\n",
    "      logVar = vae_latent[:,self.proto_vae.latent_dim:].clamp(np.log(1e-8), - np.log(1e-8))\n",
    "      z = self.proto_vae.reparameterize(mu, logVar)\n",
    "    \n",
    "    encoded_vector = self.encoder_layer(z)\n",
    "    return encoded_vector\n",
    "  def initialize_weights(self):\n",
    "    for m in self.encoder_layer.modules():\n",
    "      if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "          nn.init.zeros_(m.bias.data)\n",
    "### AENB\n",
    "class AENB(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, device, hidden_layers, activation_function=nn.ReLU):\n",
    "        super(AENB, self).__init__()\n",
    "        self.device= device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.epsilon = 1e-4\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        \n",
    "        feature_layers = []\n",
    "        previous_dim = input_dim \n",
    "        for layer_dim in self.hidden_layers:\n",
    "            feature_layers.append(nn.Linear(previous_dim, layer_dim))\n",
    "            feature_layers.append(self.activation_function())\n",
    "            # feature_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "            previous_dim = layer_dim\n",
    "        feature_layers.append(nn.Linear(previous_dim, latent_dim))\n",
    "        self.features = nn.Sequential(*feature_layers)\n",
    "        \n",
    "        decoder_layers = []\n",
    "        for layer_dim in reversed(self.hidden_layers):\n",
    "            decoder_layers.append(nn.Linear(previous_dim, layer_dim))\n",
    "            decoder_layers.append(self.activation_function())\n",
    "            # decoder_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "            previous_dim = layer_dim\n",
    "        decoder_layers.append(nn.Linear(previous_dim, input_dim * 2))\n",
    "        self.decoder_layers = nn.Sequential(*decoder_layers)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def decoder(self, z):\n",
    "        decoded = self.decoder_layers(z)\n",
    "        mu_recon = torch.exp(decoded[:, :self.input_dim]).clamp(1e-6, 1e6) \n",
    "        theta_recon = F.softplus(decoded[:, self.input_dim:]).clamp(1e-4, 1e4)  \n",
    "        return mu_recon, theta_recon\n",
    "\n",
    "    def forward(self, x, y=None, is_train=True):\n",
    "        encoded_features = self.features(x)\n",
    "        z = encoded_features\n",
    "        mu_recon, theta_recon = self.decoder(z)\n",
    "\n",
    "\n",
    "        return mu_recon, theta_recon\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "        for m in self.decoder_layers.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 01:41:26,649\tWARNING utils.py:575 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2024-04-30 01:41:28,381\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-04-30 01:41:28,572\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559517, 33538)\n",
      "Preprocessing Complete!\n",
      "(559517, 23989)\n",
      "(559517, 2000)\n",
      "(515141, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import modin.pandas as pd\n",
    "import ray\n",
    "ray.init()\n",
    "\n",
    "dat = sc.read_h5ad('/home/local/kyeonghunjeong_920205/nipa_bu/COVID19/3.analysis/9.MIL/covid19_sc/su_2020_processed.h5ad')\n",
    "print(dat.shape)\n",
    "sc.pp.filter_genes(dat, min_cells=5)\n",
    "adata_raw = dat.copy()\n",
    "sc.pp.normalize_total(dat, target_sum=1e4)\n",
    "\n",
    "print(\"Preprocessing Complete!\")\n",
    "print(dat.shape)\n",
    "\n",
    "sc.pp.log1p(dat)\n",
    "sc.pp.highly_variable_genes(dat, n_top_genes=2000)\n",
    "adata = adata_raw[:, dat.var.highly_variable]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(adata.shape)\n",
    "adata = adata[adata.obs['disease_severity_standard'].isin(['mild','moderate', 'severe'])]\n",
    "print(adata.shape)\n",
    "mapping = {'mild': 0, 'moderate': 1, 'severe': 1}\n",
    "\n",
    "adata.obs['disease_numeric'] = adata.obs['disease_severity_standard'].map(mapping)\n",
    "adata.obs['sample_id_numeric'], _ = pd.factorize(adata.obs['sample'])\n",
    "\n",
    "sample_labels = adata.obs[['disease_numeric', 'sample_id_numeric']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = '/home/local/kyeonghunjeong_920205/nipa_bu/COVID19/3.analysis/9.MIL/scAMIL_cell/WENO_su_2020_model_vae_ed128_md64_lr0.0001_500_0.1_5_15_leaktantan_fix_auc433_noflt_only2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata.__dict__['_raw'].__dict__['_var'] = adata.__dict__['_raw'].__dict__['_var'].rename(columns={'_index': 'features'})#\n",
    "adata.write(filename=f\"{saved_model_path}/anndata_proc.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.to_csv(f\"{saved_model_path}/meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = '/home/local/kyeonghunjeong_920205/nipa_bu/COVID19/3.analysis/9.MIL/scAMIL_cell/WENO_su_2020_model_vae_ed128_md64_lr0.0001_500_0.1_5_15_leaktantan_fix_auc433_noflt_only2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: When using a pre-initialized Ray cluster, please ensure that the runtime env sets environment variable __MODIN_AUTOIMPORT_PANDAS__ to 1\n",
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "for exp in range(1,9):\n",
    "    print(f'Experiment {exp}')\n",
    "    _, _, test_dataset, label_encoder, scaler = load_dataset_and_preprocessors(data_dir, exp, device)\n",
    "\n",
    "    # instance_train_dataset = update_instance_labels_with_bag_labels(train_dataset)\n",
    "    # instance_val_dataset = update_instance_labels_with_bag_labels(val_dataset)\n",
    "    instance_test_dataset = update_instance_labels_with_bag_labels(test_dataset)\n",
    "\n",
    "    model_teacher = torch.load(f'{saved_model_path}/model_teacher_exp{exp}.pt',map_location=device)\n",
    "    model_encoder = torch.load(f'{saved_model_path}/model_encoder_exp{exp}.pt',map_location=device)\n",
    "    model_student = torch.load(f'{saved_model_path}/model_student_exp{exp}.pt',map_location=device)\n",
    "\n",
    "\n",
    "    model_encoder.eval()\n",
    "    model_student.eval()\n",
    "    model_teacher.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model_encoder(instance_test_dataset.data.clone().detach().float().to(device))[:, :model_teacher.input_dims].detach().requires_grad_(False)\n",
    "        cell_score_stud = model_student(features)\n",
    "        cell_score_teacher = model_teacher.attention_module(features).squeeze(0)\n",
    "    cell_score_stud_softmax = torch.softmax(cell_score_stud, dim=1)\n",
    "    features_np = features.cpu().detach().numpy()\n",
    "    cell_score_stud_softmax_np = cell_score_stud_softmax.cpu().detach().numpy()\n",
    "    cell_score_stud_np = cell_score_stud.cpu().detach().numpy()\n",
    "    cell_score_teacher_np = cell_score_teacher.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(features_np, columns = [f'feature_{i}' for i in range(features_np.shape[1])])\n",
    "\n",
    "    df['cell_type']= label_encoder.inverse_transform(instance_test_dataset.instance_labels.cpu().detach().numpy())\n",
    "    df['cell_score_teacher'] = cell_score_teacher_np\n",
    "    df['cell_score_stud'] = cell_score_stud_np[:,1]\n",
    "    df['cell_score_stud_softmax'] = cell_score_stud_softmax_np[:,1]\n",
    "    df['bag_labels'] = instance_test_dataset.bag_labels.cpu().detach().numpy()\n",
    "    df['instance_labels'] = instance_test_dataset.instance_labels.cpu().detach().numpy()\n",
    "    df['cell_score_teacher_minmax']= (df['cell_score_teacher'].values - min(df['cell_score_teacher'].values)) / (max(df['cell_score_teacher'].values)- min(df['cell_score_teacher'].values))\n",
    "    df.to_csv(f'{saved_model_path}/cell_score_{exp}.csv', index=False)\n",
    "\n",
    "    \n",
    "    split_ratio = [0.5, 0.25, 0.25]\n",
    "    train_val_set, test_set = train_test_split(sample_labels, test_size=split_ratio[2], random_state=exp, stratify=sample_labels['disease_numeric'])\n",
    "    train_set, val_set = train_test_split(train_val_set, test_size=split_ratio[1] / (1 - split_ratio[2]), random_state=exp,stratify=train_val_set['disease_numeric'])\n",
    "    test_set.to_csv(f\"{saved_model_path}/test_set_barcodes_{exp}.csv\")\n",
    "    test_data = adata[adata.obs['sample_id_numeric'].isin(test_set['sample_id_numeric'])]    \n",
    "    test_data.obs.to_csv(f\"{saved_model_path}/obs_{exp}.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n",
      "Experiment 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = '/home/local/kyeonghunjeong_920205/nipa_bu/COVID19/3.analysis/9.MIL/scAMIL_cell/NO_Opt_student_WENO_su_2020_model_vae_ed128_md64_lr0.0001_500_0.1_500_15_NO_Opt_student_leaktantan_fix_auc4054054_noflt_only2'\n",
    "\n",
    "for exp in range(1,9):\n",
    "    print(f'Experiment {exp}')\n",
    "    _, _, test_dataset, label_encoder, scaler = load_dataset_and_preprocessors(data_dir, exp, device)\n",
    "\n",
    "    # instance_train_dataset = update_instance_labels_with_bag_labels(train_dataset)\n",
    "    # instance_val_dataset = update_instance_labels_with_bag_labels(val_dataset)\n",
    "    instance_test_dataset = update_instance_labels_with_bag_labels(test_dataset)\n",
    "\n",
    "    model_teacher = torch.load(f'{saved_model_path}/model_teacher_exp{exp}.pt',map_location=device)\n",
    "    model_encoder = torch.load(f'{saved_model_path}/model_encoder_exp{exp}.pt',map_location=device)\n",
    "    model_student = torch.load(f'{saved_model_path}/model_student_exp{exp}.pt',map_location=device)\n",
    "\n",
    "\n",
    "    model_encoder.eval()\n",
    "    model_student.eval()\n",
    "    model_teacher.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model_encoder(instance_test_dataset.data.clone().detach().float().to(device))[:, :model_teacher.input_dims].detach().requires_grad_(False)\n",
    "        cell_score_stud = model_student(features)\n",
    "        cell_score_teacher = model_teacher.attention_module(features).squeeze(0)\n",
    "    cell_score_stud_softmax = torch.softmax(cell_score_stud, dim=1)\n",
    "    features_np = features.cpu().detach().numpy()\n",
    "    cell_score_stud_softmax_np = cell_score_stud_softmax.cpu().detach().numpy()\n",
    "    cell_score_stud_np = cell_score_stud.cpu().detach().numpy()\n",
    "    cell_score_teacher_np = cell_score_teacher.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(features_np, columns = [f'feature_{i}' for i in range(features_np.shape[1])])\n",
    "\n",
    "    df['cell_type']= label_encoder.inverse_transform(instance_test_dataset.instance_labels.cpu().detach().numpy())\n",
    "    df['cell_score_teacher'] = cell_score_teacher_np\n",
    "    df['cell_score_stud'] = cell_score_stud_np[:,1]\n",
    "    df['cell_score_stud_softmax'] = cell_score_stud_softmax_np[:,1]\n",
    "    df['bag_labels'] = instance_test_dataset.bag_labels.cpu().detach().numpy()\n",
    "    df['instance_labels'] = instance_test_dataset.instance_labels.cpu().detach().numpy()\n",
    "    df['cell_score_teacher_minmax']= (df['cell_score_teacher'].values - min(df['cell_score_teacher'].values)) / (max(df['cell_score_teacher'].values)- min(df['cell_score_teacher'].values))\n",
    "    df.to_csv(f'{saved_model_path}/cell_score_{exp}.csv', index=False)\n",
    "\n",
    "    \n",
    "    split_ratio = [0.5, 0.25, 0.25]\n",
    "    train_val_set, test_set = train_test_split(sample_labels, test_size=split_ratio[2], random_state=exp, stratify=sample_labels['disease_numeric'])\n",
    "    train_set, val_set = train_test_split(train_val_set, test_size=split_ratio[1] / (1 - split_ratio[2]), random_state=exp,stratify=train_val_set['disease_numeric'])\n",
    "    test_data = adata[adata.obs['sample_id_numeric'].isin(test_set['sample_id_numeric'])]\n",
    "    test_data.obs.to_csv(f\"{saved_model_path}/obs_{exp}.csv\")\n",
    "    print(\"실제 데이터에서 훈련, 검증, 테스트 샘플 추출 완료\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # test_data.obs.columns = [sub.replace('(', '') for sub in test_data.obs.columns]\n",
    "    # test_data.obs.columns = [sub.replace(')', '') for sub in test_data.obs.columns]\n",
    "    # test_data.obs.rename(columns={'_index': 'index'}, inplace=True)\n",
    "    # test_data.__dict__['_raw'].__dict__['_var'] = adata.__dict__['_raw'].__dict__['_var'].rename(columns={'_index': 'features'})#\n",
    "    # del test_data.obs['disease_numeric']\n",
    "    # del test_data.obs['sample_id_numeric']\n",
    "    # test_data.write(filename=f\"{saved_model_path}/anndata_{exp}.h5ad\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
