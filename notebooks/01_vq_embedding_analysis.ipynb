{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ Embedding Analysis\n",
    "\n",
    "Pretrained VQ-AENB-Conditional encoder를 사용하여 전체 adata에 VQ code 및 embedding을 추출하고 분석합니다.\n",
    "\n",
    "## 출력물\n",
    "- `adata.obs['vq_code']`: 각 세포의 codebook index\n",
    "- `adata.obsm['X_vq']`: 각 세포의 quantized latent embedding\n",
    "- `adata.uns['codebook']`: codebook embedding matrix\n",
    "- `adata.uns['codebook_stats']`: code별 통계 (DataFrame → dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration - 경로 수정 필요\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "ADATA_PATH = \"/home/bmi-user/workspace/data/HSvsCD/data/Whole_SCP_PCD_Skin_805k_6k.h5ad\"\n",
    "ENCODER_PATH = \"/home/bmi-user/workspace/data/HSvsCD/scMILDQ_Cond/results/pretrained/vq_aenb_conditional_whole.pth\"\n",
    "STUDY_MAPPING_PATH = \"/home/bmi-user/workspace/data/HSvsCD/scMILDQ_Cond/results/pretrained/study_mapping.json\"\n",
    "OUTPUT_DIR = Path(\"/home/bmi-user/workspace/data/HSvsCD/scMILDQ_Cond/results/vq_analysis\")\n",
    "\n",
    "# Processing\n",
    "BATCH_SIZE = 4096\n",
    "DEVICE = \"cuda:0\"  # or \"cpu\"\n",
    "\n",
    "# Columns\n",
    "STUDY_COL = \"study\"\n",
    "STATUS_COL = \"Status\"\n",
    "ORGAN_COL = \"Organ\"\n",
    "DISEASE_COL = \"disease_numeric\"\n",
    "SAMPLE_COL = \"sample\"\n",
    "\n",
    "# Subset definitions\n",
    "SUBSETS = {\n",
    "    'whole': None,\n",
    "    'skin3': ['GSE175990', 'GSE220116'],\n",
    "    'scp1884': ['SCP1884'],\n",
    "    'skin_all': ['GSE154775', 'GSE155850', 'GSE175990', 'GSE212721', 'GSE220116'],\n",
    "    'colon_all': ['SCP1884', 'GSE225199', 'GSE260842', 'GSE277387', 'GSE114374', 'GSE116222'],\n",
    "}\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adata\n",
    "print(f\"Loading adata from: {ADATA_PATH}\")\n",
    "adata = sc.read_h5ad(ADATA_PATH)\n",
    "print(f\"Shape: {adata.n_obs} cells × {adata.n_vars} genes\")\n",
    "print(f\"Columns: {list(adata.obs.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load study mapping\n",
    "with open(STUDY_MAPPING_PATH, 'r') as f:\n",
    "    id_to_name = json.load(f)\n",
    "name_to_id = {v: int(k) for k, v in id_to_name.items()}\n",
    "print(f\"Study mapping: {name_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained encoder\n",
    "from src.models.autoencoder import VQ_AENB_Conditional\n",
    "\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "checkpoint = torch.load(ENCODER_PATH, map_location=device)\n",
    "model_config = checkpoint.get('config', {})\n",
    "print(f\"Model config: {model_config}\")\n",
    "\n",
    "encoder = VQ_AENB_Conditional(\n",
    "    input_dim=model_config['input_dim'],\n",
    "    latent_dim=model_config['latent_dim'],\n",
    "    device=device,\n",
    "    hidden_layers=model_config['hidden_layers'],\n",
    "    n_studies=model_config['n_studies'],\n",
    "    study_emb_dim=model_config.get('study_emb_dim', 16),\n",
    "    num_codes=model_config.get('num_codes', 1024),\n",
    ")\n",
    "encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "print(f\"Encoder loaded: {model_config['num_codes']} codes, {model_config['latent_dim']} latent dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cell-level VQ Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vq_embeddings(\n",
    "    adata,\n",
    "    encoder,\n",
    "    name_to_id: dict,\n",
    "    study_col: str = \"study\",\n",
    "    batch_size: int = 4096,\n",
    "    device: torch.device = None\n",
    "):\n",
    "    \"\"\"\n",
    "    adata에 VQ code와 embedding을 추가합니다 (in-place).\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object\n",
    "        encoder: Pretrained VQ-AENB-Conditional model\n",
    "        name_to_id: study name → study id mapping\n",
    "        study_col: adata.obs에서 study 정보가 있는 컬럼\n",
    "        batch_size: 배치 크기\n",
    "        device: torch device\n",
    "    \n",
    "    Adds:\n",
    "        adata.obs['vq_code']: codebook indices (int)\n",
    "        adata.obsm['X_vq']: quantized embeddings (float32)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(encoder.parameters()).device\n",
    "    \n",
    "    # Map study names to IDs\n",
    "    study_ids = adata.obs[study_col].map(name_to_id).values.astype(int)\n",
    "    \n",
    "    n_cells = adata.n_obs\n",
    "    latent_dim = encoder.latent_dim\n",
    "    \n",
    "    all_codes = np.zeros(n_cells, dtype=np.int32)\n",
    "    all_embeddings = np.zeros((n_cells, latent_dim), dtype=np.float32)\n",
    "    \n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for start_idx in tqdm(range(0, n_cells, batch_size), desc=\"Extracting VQ embeddings\"):\n",
    "            end_idx = min(start_idx + batch_size, n_cells)\n",
    "            \n",
    "            # Get batch data\n",
    "            if hasattr(adata.X, 'toarray'):\n",
    "                batch_x = torch.tensor(\n",
    "                    adata.X[start_idx:end_idx].toarray(), \n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "            else:\n",
    "                batch_x = torch.tensor(\n",
    "                    adata.X[start_idx:end_idx], \n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "            \n",
    "            batch_study_ids = torch.tensor(\n",
    "                study_ids[start_idx:end_idx], \n",
    "                dtype=torch.long, device=device\n",
    "            )\n",
    "            \n",
    "            # Get VQ codes\n",
    "            codes = encoder.get_codebook_indices(batch_x, batch_study_ids)\n",
    "            all_codes[start_idx:end_idx] = codes.cpu().numpy()\n",
    "            \n",
    "            # Get quantized embeddings\n",
    "            embeddings = encoder.features(batch_x, batch_study_ids)\n",
    "            all_embeddings[start_idx:end_idx] = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Add to adata\n",
    "    adata.obs['vq_code'] = all_codes\n",
    "    adata.obsm['X_vq'] = all_embeddings\n",
    "    \n",
    "    print(f\"Added to adata:\")\n",
    "    print(f\"  - adata.obs['vq_code']: {all_codes.shape}, unique codes: {len(np.unique(all_codes))}\")\n",
    "    print(f\"  - adata.obsm['X_vq']: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract VQ embeddings for whole adata\n",
    "extract_vq_embeddings(\n",
    "    adata=adata,\n",
    "    encoder=encoder,\n",
    "    name_to_id=name_to_id,\n",
    "    study_col=STUDY_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codebook to adata.uns\n",
    "codebook = encoder.quantizer.get_codebook().cpu().numpy()\n",
    "adata.uns['codebook'] = codebook\n",
    "print(f\"Codebook shape: {codebook.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Codebook-level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_codebook_stats(\n",
    "    adata,\n",
    "    num_codes: int,\n",
    "    status_col: str = \"Status\",\n",
    "    organ_col: str = \"Organ\",\n",
    "    study_col: str = \"study\",\n",
    "    sample_col: str = \"sample\",\n",
    "    subset_mask: np.ndarray = None,\n",
    "    subset_name: str = \"whole\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    각 VQ code별 통계를 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData with 'vq_code' in obs\n",
    "        num_codes: Total number of codes in codebook\n",
    "        status_col: Status column name\n",
    "        organ_col: Organ column name\n",
    "        study_col: Study column name\n",
    "        sample_col: Sample column name\n",
    "        subset_mask: Boolean mask for subset (None = whole)\n",
    "        subset_name: Name of subset for column prefix\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with codebook statistics\n",
    "    \"\"\"\n",
    "    if subset_mask is None:\n",
    "        obs = adata.obs\n",
    "    else:\n",
    "        obs = adata.obs[subset_mask]\n",
    "    \n",
    "    # Initialize stats dict\n",
    "    stats = {\n",
    "        'code_idx': list(range(num_codes)),\n",
    "        'n_cells': [0] * num_codes,\n",
    "        'n_samples': [0] * num_codes,  # sample diversity\n",
    "        'is_single_sample': [False] * num_codes,  # single sample flag\n",
    "    }\n",
    "    \n",
    "    # Get unique values for categorical columns\n",
    "    statuses = obs[status_col].unique()\n",
    "    organs = obs[organ_col].unique()\n",
    "    studies = obs[study_col].unique()\n",
    "    \n",
    "    # Initialize ratio columns\n",
    "    for status in statuses:\n",
    "        stats[f'status_{status}'] = [0.0] * num_codes\n",
    "    for organ in organs:\n",
    "        stats[f'organ_{organ}'] = [0.0] * num_codes\n",
    "    for study in studies:\n",
    "        stats[f'study_{study}'] = [0.0] * num_codes\n",
    "    \n",
    "    # Organ-specific disease ratios\n",
    "    stats['disease_ratio_skin'] = [np.nan] * num_codes  # HS / (HS + ctrl_skin)\n",
    "    stats['disease_ratio_colon'] = [np.nan] * num_codes  # CD / (CD + ctrl_colon)\n",
    "    stats['disease_ratio_overall'] = [0.0] * num_codes  # (CD + HS) / total\n",
    "    \n",
    "    # Group by vq_code\n",
    "    grouped = obs.groupby('vq_code')\n",
    "    \n",
    "    for code_idx, group in grouped:\n",
    "        if code_idx >= num_codes:\n",
    "            continue\n",
    "            \n",
    "        n = len(group)\n",
    "        stats['n_cells'][code_idx] = n\n",
    "        \n",
    "        # Sample diversity\n",
    "        unique_samples = group[sample_col].nunique()\n",
    "        stats['n_samples'][code_idx] = unique_samples\n",
    "        stats['is_single_sample'][code_idx] = (unique_samples == 1)\n",
    "        \n",
    "        # Status ratios\n",
    "        status_counts = group[status_col].value_counts()\n",
    "        for status in statuses:\n",
    "            stats[f'status_{status}'][code_idx] = status_counts.get(status, 0) / n\n",
    "        \n",
    "        # Organ ratios\n",
    "        organ_counts = group[organ_col].value_counts()\n",
    "        for organ in organs:\n",
    "            stats[f'organ_{organ}'][code_idx] = organ_counts.get(organ, 0) / n\n",
    "        \n",
    "        # Study ratios\n",
    "        study_counts = group[study_col].value_counts()\n",
    "        for study in studies:\n",
    "            stats[f'study_{study}'][code_idx] = study_counts.get(study, 0) / n\n",
    "        \n",
    "        # Disease ratios by organ\n",
    "        # Skin: HS / (HS + ctrl_skin)\n",
    "        n_hs = status_counts.get('HS', 0)\n",
    "        n_ctrl_skin = status_counts.get('ctrl_skin', 0)\n",
    "        if n_hs + n_ctrl_skin > 0:\n",
    "            stats['disease_ratio_skin'][code_idx] = n_hs / (n_hs + n_ctrl_skin)\n",
    "        \n",
    "        # Colon: CD / (CD + ctrl_colon)\n",
    "        n_cd = status_counts.get('CD', 0)\n",
    "        n_ctrl_colon = status_counts.get('ctrl_colon', 0)\n",
    "        if n_cd + n_ctrl_colon > 0:\n",
    "            stats['disease_ratio_colon'][code_idx] = n_cd / (n_cd + n_ctrl_colon)\n",
    "        \n",
    "        # Overall: (CD + HS) / total\n",
    "        stats['disease_ratio_overall'][code_idx] = (n_cd + n_hs) / n\n",
    "    \n",
    "    df = pd.DataFrame(stats)\n",
    "    df['subset'] = subset_name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stats for whole data\n",
    "num_codes = model_config.get('num_codes', 1024)\n",
    "\n",
    "codebook_stats_whole = compute_codebook_stats(\n",
    "    adata=adata,\n",
    "    num_codes=num_codes,\n",
    "    status_col=STATUS_COL,\n",
    "    organ_col=ORGAN_COL,\n",
    "    study_col=STUDY_COL,\n",
    "    sample_col=SAMPLE_COL,\n",
    "    subset_mask=None,\n",
    "    subset_name=\"whole\"\n",
    ")\n",
    "\n",
    "print(f\"Codebook stats shape: {codebook_stats_whole.shape}\")\n",
    "print(f\"Active codes (n_cells > 0): {(codebook_stats_whole['n_cells'] > 0).sum()}\")\n",
    "print(f\"Single-sample codes: {codebook_stats_whole['is_single_sample'].sum()}\")\n",
    "codebook_stats_whole.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of codebook usage\n",
    "active_codes = codebook_stats_whole[codebook_stats_whole['n_cells'] > 0]\n",
    "print(f\"\\n=== Codebook Usage Summary ===\")\n",
    "print(f\"Total codes: {num_codes}\")\n",
    "print(f\"Active codes: {len(active_codes)} ({len(active_codes)/num_codes*100:.1f}%)\")\n",
    "print(f\"Single-sample codes: {active_codes['is_single_sample'].sum()} ({active_codes['is_single_sample'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nCells per code:\")\n",
    "print(f\"  Mean: {active_codes['n_cells'].mean():.1f}\")\n",
    "print(f\"  Median: {active_codes['n_cells'].median():.1f}\")\n",
    "print(f\"  Max: {active_codes['n_cells'].max()}\")\n",
    "print(f\"\\nSamples per code:\")\n",
    "print(f\"  Mean: {active_codes['n_samples'].mean():.1f}\")\n",
    "print(f\"  Median: {active_codes['n_samples'].median():.1f}\")\n",
    "print(f\"  Max: {active_codes['n_samples'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stats for each subset\n",
    "all_subset_stats = [codebook_stats_whole]\n",
    "\n",
    "for subset_name, study_list in SUBSETS.items():\n",
    "    if subset_name == 'whole' or study_list is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing subset: {subset_name}\")\n",
    "    \n",
    "    # Create mask\n",
    "    mask = adata.obs[STUDY_COL].isin(study_list)\n",
    "    n_cells = mask.sum()\n",
    "    print(f\"  Cells: {n_cells}\")\n",
    "    \n",
    "    if n_cells == 0:\n",
    "        print(f\"  Skipping (no cells)\")\n",
    "        continue\n",
    "    \n",
    "    subset_stats = compute_codebook_stats(\n",
    "        adata=adata,\n",
    "        num_codes=num_codes,\n",
    "        status_col=STATUS_COL,\n",
    "        organ_col=ORGAN_COL,\n",
    "        study_col=STUDY_COL,\n",
    "        sample_col=SAMPLE_COL,\n",
    "        subset_mask=mask,\n",
    "        subset_name=subset_name\n",
    "    )\n",
    "    \n",
    "    active = (subset_stats['n_cells'] > 0).sum()\n",
    "    print(f\"  Active codes: {active}\")\n",
    "    \n",
    "    all_subset_stats.append(subset_stats)\n",
    "\n",
    "# Combine all stats\n",
    "codebook_stats_all = pd.concat(all_subset_stats, ignore_index=True)\n",
    "print(f\"\\nCombined stats shape: {codebook_stats_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by subset\n",
    "summary = codebook_stats_all.groupby('subset').agg({\n",
    "    'n_cells': 'sum',\n",
    "    'code_idx': lambda x: (codebook_stats_all.loc[x.index, 'n_cells'] > 0).sum(),\n",
    "    'is_single_sample': lambda x: x[codebook_stats_all.loc[x.index, 'n_cells'] > 0].sum()\n",
    "}).rename(columns={'code_idx': 'active_codes', 'is_single_sample': 'single_sample_codes'})\n",
    "\n",
    "print(\"=== Subset Summary ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codebook stats to adata.uns (whole stats as dict)\n",
    "# Convert DataFrame to dict for h5ad compatibility\n",
    "adata.uns['codebook_stats'] = codebook_stats_whole.to_dict(orient='list')\n",
    "print(\"Saved codebook_stats to adata.uns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adata with VQ embeddings\n",
    "output_adata_path = OUTPUT_DIR / \"adata_with_vq.h5ad\"\n",
    "print(f\"Saving adata to: {output_adata_path}\")\n",
    "adata.write_h5ad(output_adata_path)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codebook stats as CSV (all subsets)\n",
    "output_stats_path = OUTPUT_DIR / \"codebook_stats_all_subsets.csv\"\n",
    "codebook_stats_all.to_csv(output_stats_path, index=False)\n",
    "print(f\"Saved codebook stats to: {output_stats_path}\")\n",
    "\n",
    "# Save whole stats separately\n",
    "output_stats_whole_path = OUTPUT_DIR / \"codebook_stats_whole.csv\"\n",
    "codebook_stats_whole.to_csv(output_stats_whole_path, index=False)\n",
    "print(f\"Saved whole stats to: {output_stats_whole_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codebook embeddings\n",
    "output_codebook_path = OUTPUT_DIR / \"codebook_embeddings.npy\"\n",
    "np.save(output_codebook_path, codebook)\n",
    "print(f\"Saved codebook embeddings to: {output_codebook_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and verify\n",
    "print(\"Verifying saved adata...\")\n",
    "adata_reload = sc.read_h5ad(output_adata_path)\n",
    "\n",
    "print(f\"\\nadata.obs columns: {list(adata_reload.obs.columns)}\")\n",
    "print(f\"adata.obsm keys: {list(adata_reload.obsm.keys())}\")\n",
    "print(f\"adata.uns keys: {list(adata_reload.uns.keys())}\")\n",
    "\n",
    "print(f\"\\nvq_code unique values: {adata_reload.obs['vq_code'].nunique()}\")\n",
    "print(f\"X_vq shape: {adata_reload.obsm['X_vq'].shape}\")\n",
    "print(f\"codebook shape: {adata_reload.uns['codebook'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload codebook stats from uns\n",
    "codebook_stats_reloaded = pd.DataFrame(adata_reload.uns['codebook_stats'])\n",
    "print(f\"Reloaded codebook_stats shape: {codebook_stats_reloaded.shape}\")\n",
    "codebook_stats_reloaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### 저장된 파일\n",
    "1. `adata_with_vq.h5ad`: VQ code, embedding, codebook, stats가 추가된 adata\n",
    "2. `codebook_stats_all_subsets.csv`: 모든 subset의 codebook 통계\n",
    "3. `codebook_stats_whole.csv`: whole data의 codebook 통계\n",
    "4. `codebook_embeddings.npy`: codebook embedding matrix\n",
    "\n",
    "### adata 구조\n",
    "- `adata.obs['vq_code']`: 각 세포의 VQ code index\n",
    "- `adata.obsm['X_vq']`: 각 세포의 quantized latent embedding\n",
    "- `adata.uns['codebook']`: codebook embedding matrix (num_codes × latent_dim)\n",
    "- `adata.uns['codebook_stats']`: code별 통계 (dict format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
